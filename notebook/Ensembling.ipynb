{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import warnings\n",
    "\n",
    "sys.path.append(os.path.abspath('../src/'))\n",
    "from ActivationFunctions import *\n",
    "from NeuralNetwork import *\n",
    "from MyUtils import *\n",
    "from ModelSelection import *\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_norm_df = pd.read_csv('../data/divided_std_train_0_8.csv')\n",
    "test_norm_df = pd.read_csv('../data/divided_std_test_0_2.csv')\n",
    "\n",
    "tr_df = pd.read_csv('../data/divided_train_0_8.csv')\n",
    "test_df = pd.read_csv('../data/divided_test_0_2.csv')\n",
    "\n",
    "ML_cup_train = pd.read_csv('../data/ML-CUP23-TR.csv', header=None, index_col=0, comment='#')\n",
    "\n",
    "TR_INPUT = 10\n",
    "TR_OUTPUT = 3\n",
    "\n",
    "tr_normalized = tr_norm_df.values\n",
    "test_normalized = test_norm_df.values\n",
    "training_set = tr_df.values\n",
    "test_set = test_df.values\n",
    "\n",
    "scaler_out = StandardScaler()\n",
    "scaler_out.fit(ML_cup_train.values[:,TR_INPUT:])\n",
    "\n",
    "training_len = len(tr_norm_df)\n",
    "test_len = len(test_norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):    \n",
    "    with open(path, 'r') as file:\n",
    "        ret = json.load(file)\n",
    "    for el in ret:\n",
    "        with open(el['nn_file_name'], 'r') as file:\n",
    "                el['model'] = NeuralNetwork.fromJSON(file.read())\n",
    "    return ret\n",
    "            \n",
    "def save_obj(obj, path):\n",
    "    for i in obj:\n",
    "        with open(i['nn_file_name'], 'w+') as file:\n",
    "            file.write(i['model'].toJSON())\n",
    "        i['model'] = None\n",
    "    json.dump(obj, path, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_len = 32\n",
    "hidden_fun = 'sigmoid'\n",
    "output_fun = 'identity'\n",
    "sigmoid_l1 = create_stratified_topology([TR_INPUT,hidden_len,TR_OUTPUT], \n",
    "                                      [[None,[]]]*TR_INPUT + [[hidden_fun, [1]]]*hidden_len + [[output_fun, []]]*TR_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):    \n",
    "    with open(path, 'r') as file:\n",
    "        ret = json.load(file)\n",
    "    for el in ret:\n",
    "        with open(el['nn_file_name'], 'r') as file:\n",
    "                el['model'] = NeuralNetwork.fromJSON(file.read())\n",
    "    return ret\n",
    "            \n",
    "def save_obj(obj, path):\n",
    "    for i in obj:\n",
    "        with open(i['nn_file_name'], 'w+') as file:\n",
    "            file.write(i['model'].toJSON())\n",
    "        i['model'] = None\n",
    "    json.dump(obj, path, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_val(x):\n",
    "    a =['learning_rate', 'lr_decay_tau', 'alpha_momentum']\n",
    "    b =['adamax_learning_rate', 'exp_decay_rate_1', 'exp_decay_rate_2']\n",
    "    if x['adamax']:\n",
    "        for i in a:\n",
    "            if i in x.keys():\n",
    "                x[i] = None\n",
    "    else:\n",
    "        for i in b:\n",
    "            if i in x.keys():\n",
    "                x[i] = None   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = ['topology', 'stats',\n",
    " 'batch_size',\n",
    " 'min_epochs',\n",
    " 'max_epochs',\n",
    " 'patience',\n",
    " 'error_increase_tolerance',\n",
    " 'lambda_tikhonov',\n",
    " \n",
    " 'learning_rate',\n",
    " 'alpha_momentum',\n",
    " 'lr_decay_tau',\n",
    " \n",
    " 'adamax',\n",
    " 'adamax_learning_rate',\n",
    " 'exp_decay_rate_1',\n",
    " 'exp_decay_rate_2',\n",
    " \n",
    " 'mean_mean_euclidean_error',\n",
    " 'mean_mean_squared_error',\n",
    " 'var_mean_euclidean_error',\n",
    " 'var_mean_squared_error',\n",
    " 'mean_best_validation_training_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_files = ['bagging_model.csv']\n",
    "topologies = ['32_sigmoid']\n",
    "folder = '../data/gs_data/'\n",
    "topologies_dict = {}\n",
    "gs_results = []\n",
    "for i, f in enumerate(results_files):\n",
    "    if os.path.isfile(folder+ f):\n",
    "        dummy = pd.read_csv(folder + f)\n",
    "        topologies_dict[topologies[i]] = ast.literal_eval(dummy['topology'][0])\n",
    "        dummy['topology'] = topologies[i]\n",
    "        \n",
    "        gs_results.append(dummy)\n",
    "    \n",
    "\n",
    "\n",
    "orig_df = pd.concat(gs_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = [x for x in columns_order if x in orig_df.columns]\n",
    "orig_df = orig_df[columns_order]\n",
    "\n",
    "order_by = 'mean_mean_euclidean_error'\n",
    "orig_df.sort_values(by=[order_by], inplace=True)\n",
    "orig_df = orig_df.reset_index(drop=True)\n",
    "gs_results = orig_df.drop(['stats'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results = gs_results.apply(remove_useless_val, axis=1)\n",
    "for i in gs_results.columns[1:]:\n",
    "    gs_results[i] = gs_results[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = list(gs_results.columns)\n",
    "numerical_col.remove('topology')\n",
    "st_opt_col = ['learning_rate','lr_decay_tau','alpha_momentum']\n",
    "adamax_opt_col = ['adamax','adamax_learning_rate','exp_decay_rate_1','exp_decay_rate_2']\n",
    "metrics_col = [x for x in gs_results.columns if x.startswith(('var', 'mean'))]\n",
    "general_col = [item for item in list(gs_results.columns) if item not in st_opt_col and item not in metrics_col and item not in adamax_opt_col]\n",
    "tr_input_col = [item for item in list(gs_results.columns) if item not in metrics_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topology</th>\n",
       "      <th>stats</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>min_epochs</th>\n",
       "      <th>max_epochs</th>\n",
       "      <th>patience</th>\n",
       "      <th>error_increase_tolerance</th>\n",
       "      <th>lambda_tikhonov</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>alpha_momentum</th>\n",
       "      <th>lr_decay_tau</th>\n",
       "      <th>adamax</th>\n",
       "      <th>mean_mean_euclidean_error</th>\n",
       "      <th>mean_mean_squared_error</th>\n",
       "      <th>var_mean_euclidean_error</th>\n",
       "      <th>var_mean_squared_error</th>\n",
       "      <th>mean_best_validation_training_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32_sigmoid</td>\n",
       "      <td>{'adamax': False, 'exp_decay_rate_2': 0.999, '...</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.85</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "      <td>0.098424</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>5.119440e-07</td>\n",
       "      <td>0.009886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topology                                              stats  batch_size  \\\n",
       "0  32_sigmoid  {'adamax': False, 'exp_decay_rate_2': 0.999, '...           8   \n",
       "\n",
       "   min_epochs  max_epochs  patience  error_increase_tolerance  \\\n",
       "0         150         500         5                  0.000001   \n",
       "\n",
       "   lambda_tikhonov  learning_rate  alpha_momentum  lr_decay_tau  adamax  \\\n",
       "0     1.000000e-09           0.11            0.85           200   False   \n",
       "\n",
       "   mean_mean_euclidean_error  mean_mean_squared_error  \\\n",
       "0                   0.098424                 0.013096   \n",
       "\n",
       "   var_mean_euclidean_error  var_mean_squared_error  \\\n",
       "0                  0.000005            5.119440e-07   \n",
       "\n",
       "   mean_best_validation_training_error  \n",
       "0                             0.009886  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = np.random.default_rng(seed=None)\n",
    "def get_new_tr_vl(pattern_set, len_ds, gen):\n",
    "    return gen.choice(pattern_set, len_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 32\n",
    "tr_len = 533\n",
    "max_epochs = 500\n",
    "mod = []\n",
    "\n",
    "met = [ErrorFunctions.mean_squared_error, ErrorFunctions.mean_euclidean_error, ]\n",
    "predictions_accumul_tr = np.zeros((max_epochs, training_len, TR_OUTPUT))\n",
    "predictions_accumul_val = np.zeros((max_epochs, test_len, TR_OUTPUT))\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    tr = get_new_tr_vl(tr_normalized, 533, gen)\n",
    "    NN = NeuralNetwork(sigmoid_l1, -0.75, 0.75, True, (i + 69))\n",
    "    stats = NN.train(training_set = tr, \n",
    "                    validation_set = test_normalized, \n",
    "                    \n",
    "                    batch_size= 8, \n",
    "                    max_epochs= max_epochs, \n",
    "                    min_epochs= 150,\n",
    "                    retrainig_es_error = orig_df.iloc[0]['mean_best_validation_training_error'],\n",
    "                    patience = 5, \n",
    "                    error_increase_tolerance = 0.000001, \n",
    "                    \n",
    "                    lambda_tikhonov = 1.000000e-09, # off\n",
    "                    \n",
    "                    adamax = False,\n",
    "                    \n",
    "                    learning_rate = 0.11/8,\n",
    "                    lr_decay_tau = 200, # off\n",
    "                    eta_tau= (0.11/8)*0.01,\n",
    "                    alpha_momentum = 0.85, # off\n",
    "                    nesterov = False,\n",
    "                    \n",
    "                    metrics = [ErrorFunctions.mean_squared_error, ErrorFunctions.mean_euclidean_error, ], \n",
    "                    collect_data=True, \n",
    "                    collect_data_batch=False, \n",
    "                    verbose=True,\n",
    "                    \n",
    "                    dataset_agg = tr_normalized)\n",
    "\n",
    "        \n",
    "    predictions_accumul_tr += np.array(stats['training_pred'] + \n",
    "                                            [stats['training_pred'][-1]]*(max_epochs - stats['epochs']))\n",
    "\n",
    "    \n",
    "    predictions_accumul_val += np.array(stats['validation_pred'] + \n",
    "                                              [stats['validation_pred'][-1]]*(max_epochs - stats['epochs']))\n",
    "\n",
    "    \n",
    "    mod.append([NN, stats, tr])\n",
    "    with open('../data/net/models_6_ens/model_' + str(i) + '.json', 'w+') as file:\n",
    "            file.write(NN.toJSON())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 800, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_accumul_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_obj(list_mod):\n",
    "    interesting_model = []\n",
    "    for i in range(len(list_mod)):\n",
    "        \n",
    "        dummy = {}\n",
    "        dummy['nn_file_name'] = '../data/net/models_6_ens/model_' + str(i) + '.json'\n",
    "        dummy['index'] = i\n",
    "        dummy['model'] = list_mod[i][0]\n",
    "        dummy['stats'] = list_mod[i][1]\n",
    "        dummy['top_name'] = '32_sigmoid'\n",
    "        \n",
    "        dummy['std_prediction_tr'] = dummy['model'].predict_array(list_mod[i][2][:,:TR_INPUT])\n",
    "        dummy['std_prediction_test'] = dummy['model'].predict_array(test_normalized[:,:TR_INPUT])\n",
    "        \n",
    "        dummy['prediction_tr'] = scaler_out.inverse_transform(dummy['std_prediction_tr'])\n",
    "        dummy['prediction_test'] = scaler_out.inverse_transform(dummy['std_prediction_test'])\n",
    "\n",
    "        dummy['std_tr_error'] = ErrorFunctions.mean_euclidean_error(dummy['std_prediction_tr'], list_mod[i][2][:,TR_INPUT:])\n",
    "        dummy['std_test_error'] = ErrorFunctions.mean_euclidean_error(dummy['std_prediction_test'], test_normalized[:,TR_INPUT:])\n",
    "             \n",
    "        dummy['tr_error'] = ErrorFunctions.mean_euclidean_error(dummy['prediction_tr'], scaler_out.inverse_transform(list_mod[i][2][:,TR_INPUT:]))\n",
    "        dummy['test_error'] = ErrorFunctions.mean_euclidean_error(dummy['prediction_test'], test_set[:,TR_INPUT:])\n",
    "        interesting_model.append(dummy)\n",
    "        \n",
    "    return interesting_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = construct_obj(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/net/models_6_ens/models.json', 'w+') as file:\n",
    "    save_obj(models_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/net/models_6_ens/ens_curves_tr.json', 'w+') as file:\n",
    "    file.write(json.dumps(predictions_accumul_tr.tolist()))\n",
    "with open('../data/net/models_6_ens/ens_curves_test.json', 'w+') as file:\n",
    "    file.write(json.dumps(predictions_accumul_val.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
