{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import ast\n",
    "\n",
    "sys.path.append(os.path.abspath('../src/'))\n",
    "from ActivationFunctions import *\n",
    "from NeuralNetwork import *\n",
    "from MyUtils import *\n",
    "from ModelSelection import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_norm_df = pd.read_csv('../data/divided_std_train_0_8.csv')\n",
    "test_norm_df = pd.read_csv('../data/divided_std_test_0_2.csv')\n",
    "\n",
    "tr_df = pd.read_csv('../data/divided_train_0_8.csv')\n",
    "test_df = pd.read_csv('../data/divided_test_0_2.csv')\n",
    "\n",
    "ML_cup_train = pd.read_csv('../data/ML-CUP23-TR.csv', header=None, index_col=0, comment='#').sample(frac=1, random_state=RANDOM_STATE)\n",
    "TR_INPUT = 10\n",
    "TR_OUTPUT = 3\n",
    "scaler_in = StandardScaler()\n",
    "scaler_out = StandardScaler()\n",
    "ML_cup_train_norm = ML_cup_train.values.copy()\n",
    "\n",
    "ML_cup_train_norm[:,:TR_INPUT] = scaler_in.fit_transform(ML_cup_train_norm[:,:TR_INPUT])\n",
    "ML_cup_train_norm[:,TR_INPUT:] = scaler_out.fit_transform(ML_cup_train_norm[:,TR_INPUT:])\n",
    "\n",
    "training_len = len(ML_cup_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):    \n",
    "    with open(path, 'r') as file:\n",
    "        ret = json.load(file)\n",
    "    for el in ret:\n",
    "        with open(el['nn_file_name'], 'r') as file:\n",
    "                el['model'] = NeuralNetwork.fromJSON(file.read())\n",
    "    return ret\n",
    "            \n",
    "def save_obj(obj, path):\n",
    "    for i in obj:\n",
    "        with open(i['nn_file_name'], 'w+') as file:\n",
    "            file.write(i['model'].toJSON())\n",
    "        i['model'] = None\n",
    "    json.dump(obj, path, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_len = 32\n",
    "hidden_fun = 'sigmoid'\n",
    "output_fun = 'identity'\n",
    "sigmoid_l1 = create_stratified_topology([TR_INPUT,hidden_len,TR_OUTPUT], \n",
    "                                      [[None,[]]]*TR_INPUT + [[hidden_fun, [1]]]*hidden_len + [[output_fun, []]]*TR_OUTPUT)\n",
    "NeuralNetwork.display_topology(sigmoid_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):    \n",
    "    with open(path, 'r') as file:\n",
    "        ret = json.load(file)\n",
    "    for el in ret:\n",
    "        with open(el['nn_file_name'], 'r') as file:\n",
    "                el['model'] = NeuralNetwork.fromJSON(file.read())\n",
    "    return ret\n",
    "            \n",
    "def save_obj(obj, path):\n",
    "    for i in obj:\n",
    "        with open(i['nn_file_name'], 'w+') as file:\n",
    "            file.write(i['model'].toJSON())\n",
    "        i['model'] = None\n",
    "    json.dump(obj, path, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_val(x):\n",
    "    a =['learning_rate', 'lr_decay_tau', 'alpha_momentum']\n",
    "    b =['adamax_learning_rate', 'exp_decay_rate_1', 'exp_decay_rate_2']\n",
    "    if x['adamax']:\n",
    "        for i in a:\n",
    "            if i in x.keys():\n",
    "                x[i] = None\n",
    "    else:\n",
    "        for i in b:\n",
    "            if i in x.keys():\n",
    "                x[i] = None   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = ['topology', 'stats',\n",
    " 'batch_size',\n",
    " 'min_epochs',\n",
    " 'max_epochs',\n",
    " 'patience',\n",
    " 'error_increase_tolerance',\n",
    " 'lambda_tikhonov',\n",
    " \n",
    " 'learning_rate',\n",
    " 'alpha_momentum',\n",
    " 'lr_decay_tau',\n",
    " \n",
    " 'adamax',\n",
    " 'adamax_learning_rate',\n",
    " 'exp_decay_rate_1',\n",
    " 'exp_decay_rate_2',\n",
    " \n",
    " 'mean_mean_euclidean_error',\n",
    " 'mean_mean_squared_error',\n",
    " 'var_mean_euclidean_error',\n",
    " 'var_mean_squared_error',\n",
    " 'mean_best_validation_training_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_files = ['bagging_sub_model.csv']\n",
    "topologies = ['32_sigmoid']\n",
    "folder = '../data/gs_data/'\n",
    "topologies_dict = {}\n",
    "gs_results = []\n",
    "for i, f in enumerate(results_files):\n",
    "    if os.path.isfile(folder+ f):\n",
    "        dummy = pd.read_csv(folder + f)\n",
    "        topologies_dict[topologies[i]] = ast.literal_eval(dummy['topology'][0])\n",
    "        dummy['topology'] = topologies[i]\n",
    "        \n",
    "        gs_results.append(dummy)\n",
    "    \n",
    "\n",
    "\n",
    "orig_df = pd.concat(gs_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = [x for x in columns_order if x in orig_df.columns]\n",
    "orig_df = orig_df[columns_order]\n",
    "\n",
    "order_by = 'mean_mean_euclidean_error'\n",
    "orig_df.sort_values(by=[order_by], inplace=True)\n",
    "orig_df = orig_df.reset_index(drop=True)\n",
    "gs_results = orig_df.drop(['stats'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results = gs_results.apply(remove_useless_val, axis=1)\n",
    "for i in gs_results.columns[1:]:\n",
    "    gs_results[i] = gs_results[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = list(gs_results.columns)\n",
    "numerical_col.remove('topology')\n",
    "st_opt_col = ['learning_rate','lr_decay_tau','alpha_momentum']\n",
    "adamax_opt_col = ['adamax','adamax_learning_rate','exp_decay_rate_1','exp_decay_rate_2']\n",
    "metrics_col = [x for x in gs_results.columns if x.startswith(('var', 'mean'))]\n",
    "general_col = [item for item in list(gs_results.columns) if item not in st_opt_col and item not in metrics_col and item not in adamax_opt_col]\n",
    "tr_input_col = [item for item in list(gs_results.columns) if item not in metrics_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 33\n",
    "tr_len = 533\n",
    "max_epochs = 500\n",
    "mod = []\n",
    "\n",
    "met = [ErrorFunctions.mean_squared_error, ErrorFunctions.mean_euclidean_error, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    interesting_model = load_obj('../data/net/models_7_final_retr/models.json')\n",
    "    interesting_model_2 = load_obj('../data/net/a1/models.json')\n",
    "    interesting_model_3 = load_obj('../data/net/a2/models.json')\n",
    "    interesting_model_4 = load_obj('../data/net/a3/models.json')\n",
    "    interesting_model_5 = load_obj('../data/net/a4/models.json')\n",
    "\n",
    "    scarto = 4\n",
    "    for i, el in enumerate(interesting_model_2):\n",
    "        el['nn_file_name'] = '../data/net/models_7_final_retr/model_' + str(i + scarto) + '.json'\n",
    "        el['index'] = i + scarto\n",
    "    interesting_model += interesting_model_2\n",
    "    scarto = 4 + 11\n",
    "    for i, el in enumerate(interesting_model_3):\n",
    "        el['nn_file_name'] = '../data/net/models_7_final_retr/model_' + str(i + scarto) + '.json'\n",
    "        el['index'] = i + scarto\n",
    "    interesting_model += interesting_model_3\n",
    "    scarto = 4 + 11 + 11\n",
    "    for i, el in enumerate(interesting_model_4):\n",
    "        el['nn_file_name'] = '../data/net/models_7_final_retr/model_' + str(i + scarto) + '.json'\n",
    "        el['index'] = i + scarto\n",
    "    interesting_model += interesting_model_4\n",
    "    scarto = 4 + 11 + 11 + 3\n",
    "    for i, el in enumerate(interesting_model_5):\n",
    "        el['nn_file_name'] = '../data/net/models_7_final_retr/model_' + str(i + scarto) + '.json'\n",
    "        el['index'] = i + scarto\n",
    "    interesting_model += interesting_model_5\n",
    "\n",
    "    with open('../data/net/models_7_final_retr/ens_curves_tr.json', 'r') as file:\n",
    "        predictions_accumul_tr = np.array(json.load(file))\n",
    "\n",
    "    with open('../data/net/a1/ens_curves_tr.json', 'r') as file:\n",
    "        predictions_accumul_tr += np.array(json.load(file))\n",
    "        \n",
    "    with open('../data/net/a2/ens_curves_tr.json', 'r') as file:\n",
    "        predictions_accumul_tr += np.array(json.load(file))\n",
    "        \n",
    "    with open('../data/net/a3/ens_curves_tr.json', 'r') as file:\n",
    "        predictions_accumul_tr += np.array(json.load(file))\n",
    "        \n",
    "    with open('../data/net/a4/ens_curves_tr.json', 'r') as file:\n",
    "        predictions_accumul_tr += np.array(json.load(file))\n",
    "        \n",
    "    with open('../data/net/models_7_final_retr/models.json', 'w+') as file:\n",
    "        save_obj(interesting_model, file)\n",
    "        \n",
    "    with open('../data/net/models_7_final_retr/ens_curves_tr.json', 'w+') as file:\n",
    "        file.write(json.dumps(predictions_accumul_tr.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_model = load_obj('../data/net/models_7_final_retr/models.json')\n",
    "with open('../data/net/models_7_final_retr/ens_curves_tr.json', 'r') as file:\n",
    "    predictions_accumul_tr = np.array(json.load(file))/n_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemlbe(compl_obj, set):\n",
    "\n",
    "    output = []#np.empty((len(set), TR_OUTPUT))\n",
    "    \n",
    "    for el in set:\n",
    "        dummy = np.zeros(TR_OUTPUT)\n",
    "        for j in range(len(compl_obj)):\n",
    "            dummy += compl_obj[j]['model'].predict(el)\n",
    "\n",
    "        output.append(dummy/len(compl_obj))\n",
    "        #print(dummy/len(compl_obj))\n",
    "        \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_prediction_tr = ensemlbe(interesting_model, ML_cup_train_norm[:,:TR_INPUT])\n",
    "\n",
    "prediction_tr = scaler_out.inverse_transform(str_prediction_tr)\n",
    "\n",
    "std_error_tr = ErrorFunctions.mean_euclidean_error(str_prediction_tr, ML_cup_train_norm[:,TR_INPUT:])\n",
    "\n",
    "error_tr = ErrorFunctions.mean_euclidean_error(prediction_tr, ML_cup_train.values[:,TR_INPUT:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('std_error_tr:', std_error_tr)\n",
    "print('error_tr:', error_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_err_train = [[], []]\n",
    "\n",
    "for i in range(len(met)):\n",
    "    for epoch in range(max_epochs):\n",
    "        ens_err_train[i].append(met[i](predictions_accumul_tr[epoch], ML_cup_train_norm[:,TR_INPUT:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, figsize=(5,4))\n",
    "fig.suptitle('MEE Learning Curves Ensembling And Sub-models')\n",
    "\n",
    "for dummy in interesting_model:\n",
    "    stats = dummy['stats']\n",
    "        \n",
    "    ax1.plot(list(range(len(stats['training_mean_euclidean_error']))), stats['training_mean_euclidean_error'], color = 'C0', alpha=0.05)\n",
    "   \n",
    "    \n",
    "ax1.plot(list(range(len(ens_err_train[1]))), ens_err_train[1], color = 'C0', label='training_error')\n",
    "\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_cup_test = pd.read_csv('../data/ML-CUP23-TS.csv', header=None, index_col=0, comment='#')\n",
    "ML_cup_train = pd.read_csv('../data/ML-CUP23-TR.csv', header=None, index_col=0, comment='#')\n",
    "\n",
    "in_scaler = StandardScaler()\n",
    "in_scaler.fit(ML_cup_train.values[:,:TR_INPUT])\n",
    "\n",
    "ML_cup_test_std = in_scaler.transform(ML_cup_test)\n",
    "ML_cup_test_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_cup_pred_std = ensemlbe(interesting_model, ML_cup_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_cup_pred_df = pd.DataFrame(scaler_out.inverse_transform(ML_cup_pred_std))\n",
    "ML_cup_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_cup_pred_df.index += 1\n",
    "ML_cup_pred_df.to_csv('../data/AIdra_ML-CUP23-TS.csv', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
