{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import ast\n",
    "\n",
    "sys.path.append(os.path.abspath('../src/'))\n",
    "from ActivationFunctions import *\n",
    "from NeuralNetwork import *\n",
    "from MyUtils import *\n",
    "from ModelSelection import *\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_norm_df = pd.read_csv('../data/divided_std_train_0_8.csv')\n",
    "test_norm_df = pd.read_csv('../data/divided_std_test_0_2.csv')\n",
    "\n",
    "tr_df = pd.read_csv('../data/divided_train_0_8.csv')\n",
    "test_df = pd.read_csv('../data/divided_test_0_2.csv')\n",
    "\n",
    "ML_cup_train = pd.read_csv('../data/ML-CUP23-TR.csv', header=None, index_col=0, comment='#').sample(frac=1, random_state=RANDOM_STATE)\n",
    "TR_INPUT = 10\n",
    "TR_OUTPUT = 3\n",
    "scaler_in = StandardScaler()\n",
    "scaler_out = StandardScaler()\n",
    "ML_cup_train_norm = ML_cup_train.values.copy()\n",
    "\n",
    "ML_cup_train_norm[:,:TR_INPUT] = scaler_in.fit_transform(ML_cup_train_norm[:,:TR_INPUT])\n",
    "ML_cup_train_norm[:,TR_INPUT:] = scaler_out.fit_transform(ML_cup_train_norm[:,TR_INPUT:])\n",
    "\n",
    "training_len = len(ML_cup_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):    \n",
    "    with open(path, 'r') as file:\n",
    "        ret = json.load(file)\n",
    "    for el in ret:\n",
    "        with open(el['nn_file_name'], 'r') as file:\n",
    "                el['model'] = NeuralNetwork.fromJSON(file.read())\n",
    "    return ret\n",
    "            \n",
    "def save_obj(obj, path):\n",
    "    for i in obj:\n",
    "        with open(i['nn_file_name'], 'w+') as file:\n",
    "            file.write(i['model'].toJSON())\n",
    "        i['model'] = None\n",
    "    json.dump(obj, path, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_len = 32\n",
    "hidden_fun = 'sigmoid'\n",
    "output_fun = 'identity'\n",
    "sigmoid_l1 = create_stratified_topology([TR_INPUT,hidden_len,TR_OUTPUT], \n",
    "                                      [[None,[]]]*TR_INPUT + [[hidden_fun, [1]]]*hidden_len + [[output_fun, []]]*TR_OUTPUT)\n",
    "NeuralNetwork.display_topology(sigmoid_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):    \n",
    "    with open(path, 'r') as file:\n",
    "        ret = json.load(file)\n",
    "    for el in ret:\n",
    "        with open(el['nn_file_name'], 'r') as file:\n",
    "                el['model'] = NeuralNetwork.fromJSON(file.read())\n",
    "    return ret\n",
    "            \n",
    "def save_obj(obj, path):\n",
    "    for i in obj:\n",
    "        with open(i['nn_file_name'], 'w+') as file:\n",
    "            file.write(i['model'].toJSON())\n",
    "        i['model'] = None\n",
    "    json.dump(obj, path, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_val(x):\n",
    "    a =['learning_rate', 'lr_decay_tau', 'alpha_momentum']\n",
    "    b =['adamax_learning_rate', 'exp_decay_rate_1', 'exp_decay_rate_2']\n",
    "    if x['adamax']:\n",
    "        for i in a:\n",
    "            if i in x.keys():\n",
    "                x[i] = None\n",
    "    else:\n",
    "        for i in b:\n",
    "            if i in x.keys():\n",
    "                x[i] = None   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = ['topology', 'stats',\n",
    " 'batch_size',\n",
    " 'min_epochs',\n",
    " 'max_epochs',\n",
    " 'patience',\n",
    " 'error_increase_tolerance',\n",
    " 'lambda_tikhonov',\n",
    " \n",
    " 'learning_rate',\n",
    " 'alpha_momentum',\n",
    " 'lr_decay_tau',\n",
    " \n",
    " 'adamax',\n",
    " 'adamax_learning_rate',\n",
    " 'exp_decay_rate_1',\n",
    " 'exp_decay_rate_2',\n",
    " \n",
    " 'mean_mean_euclidean_error',\n",
    " 'mean_mean_squared_error',\n",
    " 'var_mean_euclidean_error',\n",
    " 'var_mean_squared_error',\n",
    " 'mean_best_validation_training_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_files = ['bagging_sub_model.csv']\n",
    "topologies = ['32_sigmoid']\n",
    "folder = '../data/gs_data/'\n",
    "topologies_dict = {}\n",
    "gs_results = []\n",
    "for i, f in enumerate(results_files):\n",
    "    if os.path.isfile(folder+ f):\n",
    "        dummy = pd.read_csv(folder + f)\n",
    "        topologies_dict[topologies[i]] = ast.literal_eval(dummy['topology'][0])\n",
    "        dummy['topology'] = topologies[i]\n",
    "        \n",
    "        gs_results.append(dummy)\n",
    "\n",
    "orig_df = pd.concat(gs_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = [x for x in columns_order if x in orig_df.columns]\n",
    "orig_df = orig_df[columns_order]\n",
    "\n",
    "order_by = 'mean_mean_euclidean_error'\n",
    "orig_df.sort_values(by=[order_by], inplace=True)\n",
    "orig_df = orig_df.reset_index(drop=True)\n",
    "gs_results = orig_df.drop(['stats'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results = gs_results.apply(remove_useless_val, axis=1)\n",
    "for i in gs_results.columns[1:]:\n",
    "    gs_results[i] = gs_results[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = list(gs_results.columns)\n",
    "numerical_col.remove('topology')\n",
    "st_opt_col = ['learning_rate','lr_decay_tau','alpha_momentum']\n",
    "adamax_opt_col = ['adamax','adamax_learning_rate','exp_decay_rate_1','exp_decay_rate_2']\n",
    "metrics_col = [x for x in gs_results.columns if x.startswith(('var', 'mean'))]\n",
    "general_col = [item for item in list(gs_results.columns) if item not in st_opt_col and item not in metrics_col and item not in adamax_opt_col]\n",
    "tr_input_col = [item for item in list(gs_results.columns) if item not in metrics_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = np.random.default_rng(seed=None)\n",
    "def get_new_tr_vl(pattern_set, len_ds, gen):\n",
    "    return gen.choice(pattern_set, len_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 32\n",
    "tr_len = 533\n",
    "max_epochs = 500\n",
    "mod = []\n",
    "\n",
    "met = [ErrorFunctions.mean_squared_error, ErrorFunctions.mean_euclidean_error, ]\n",
    "predictions_accumul_tr = np.zeros((max_epochs, training_len, TR_OUTPUT))\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    tr = get_new_tr_vl(ML_cup_train_norm, 533, gen)\n",
    "    NN = NeuralNetwork(sigmoid_l1, -0.75, 0.75, True, (i + 90129090))\n",
    "    stats = NN.train(training_set = tr, \n",
    "                    validation_set = None, \n",
    "                    \n",
    "                    batch_size= 8, \n",
    "                    max_epochs= max_epochs, \n",
    "                    min_epochs= 150,\n",
    "                    retrainig_es_error = orig_df.iloc[0]['mean_best_validation_training_error'],\n",
    "                    patience = 5, \n",
    "                    error_increase_tolerance = 0.000001, \n",
    "                    \n",
    "                    lambda_tikhonov = 1.000000e-09, # off\n",
    "                    \n",
    "                    adamax = False,\n",
    "                    \n",
    "                    learning_rate = 0.11/8,\n",
    "                    lr_decay_tau = 200, # off\n",
    "                    eta_tau= (0.11/8)*0.01,\n",
    "                    alpha_momentum = 0.85, # off\n",
    "                    nesterov = False,\n",
    "                    \n",
    "                    metrics = [ErrorFunctions.mean_squared_error, ErrorFunctions.mean_euclidean_error, ], \n",
    "                    collect_data=True, \n",
    "                    collect_data_batch=False, \n",
    "                    verbose=True,\n",
    "                    \n",
    "                    supp_dataset = ML_cup_train_norm)\n",
    "\n",
    "        \n",
    "    predictions_accumul_tr += np.array(stats['training_pred'] + \n",
    "                                            [stats['training_pred'][-1]]*(max_epochs - stats['epochs']))\n",
    "\n",
    "    \n",
    "    mod.append([NN, stats, tr])\n",
    "    with open('../data/net/models_7_final_retr/model_' + str(i) + '.json', 'w+') as file:\n",
    "            file.write(NN.toJSON())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_obj(list_mod):\n",
    "    interesting_model = []\n",
    "    for i in range(len(list_mod)):\n",
    "        \n",
    "        dummy = {}\n",
    "        dummy['nn_file_name'] = '../data/net/models_7_final_retr/model_' + str(i) + '.json'\n",
    "        dummy['index'] = i\n",
    "        dummy['model'] = list_mod[i][0]\n",
    "        dummy['stats'] = list_mod[i][1]\n",
    "        dummy['top_name'] = '32_sigmoid'\n",
    "        \n",
    "        dummy['std_prediction_tr'] = dummy['model'].predict_array(list_mod[i][2][:,:TR_INPUT])      \n",
    "        dummy['prediction_tr'] = scaler_out.inverse_transform(dummy['std_prediction_tr'])\n",
    "        dummy['std_tr_error'] = ErrorFunctions.mean_euclidean_error(dummy['std_prediction_tr'], list_mod[i][2][:,TR_INPUT:])\n",
    "             \n",
    "        dummy['tr_error'] = ErrorFunctions.mean_euclidean_error(dummy['prediction_tr'], scaler_out.inverse_transform(list_mod[i][2][:,TR_INPUT:]))\n",
    "        interesting_model.append(dummy)\n",
    "        \n",
    "    return interesting_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = construct_obj(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/net/models_7_final_retr/models.json', 'w+') as file:\n",
    "    save_obj(models_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/net/models_7_final_retr/ens_curves_tr.json', 'w+') as file:\n",
    "    file.write(json.dumps(predictions_accumul_tr.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
