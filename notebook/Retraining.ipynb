{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import ast\n",
    "import warnings\n",
    "\n",
    "sys.path.append(os.path.abspath('../src/'))\n",
    "from ActivationFunctions import *\n",
    "from NeuralNetwork import *\n",
    "from MyUtils import *\n",
    "from ModelSelection import *\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_norm_df = pd.read_csv('../data/divided_std_train_0_8.csv')\n",
    "test_norm_df = pd.read_csv('../data/divided_std_test_0_2.csv')\n",
    "\n",
    "tr_df = pd.read_csv('../data/divided_train_0_8.csv')\n",
    "test_df = pd.read_csv('../data/divided_test_0_2.csv')\n",
    "\n",
    "TR_INPUT = 10\n",
    "TR_OUTPUT = 3\n",
    "\n",
    "tr_normalized = tr_norm_df.values\n",
    "test_normalized = test_norm_df.values\n",
    "training_set = tr_df.values\n",
    "test_set = test_df.values\n",
    "\n",
    "data_set_normalized = numpy.append(tr_normalized, test_normalized, axis=0)\n",
    "\n",
    "scaler_out = StandardScaler()\n",
    "scaler_out.fit(training_set[:,TR_INPUT:])\n",
    "\n",
    "training_len = len(tr_norm_df)\n",
    "test_len = len(test_norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):    \n",
    "    with open(path, 'r') as file:\n",
    "        ret = json.load(file)\n",
    "    for el in ret:\n",
    "        with open(el['nn_file_name'], 'r') as file:\n",
    "                el['model'] = NeuralNetwork.fromJSON(file.read())\n",
    "    return ret\n",
    "            \n",
    "def save_obj(obj, path):\n",
    "    for i in obj:\n",
    "        with open(i['nn_file_name'], 'w+') as file:\n",
    "            file.write(i['model'].toJSON())\n",
    "        i['model'] = None\n",
    "    json.dump(obj, path, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_len = 32\n",
    "hidden_fun = 'sigmoid'\n",
    "output_fun = 'identity'\n",
    "sigmoid_l1 = create_stratified_topology([TR_INPUT,hidden_len,TR_OUTPUT], \n",
    "                                      [[None,[]]]*TR_INPUT + [[hidden_fun, [1]]]*hidden_len + [[output_fun, []]]*TR_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(path):    \n",
    "    with open(path, 'r') as file:\n",
    "        ret = json.load(file)\n",
    "    for el in ret:\n",
    "        with open(el['nn_file_name'], 'r') as file:\n",
    "                el['model'] = NeuralNetwork.fromJSON(file.read())\n",
    "    return ret\n",
    "            \n",
    "def save_obj(obj, path):\n",
    "    for i in obj:\n",
    "        with open(i['nn_file_name'], 'w+') as file:\n",
    "            file.write(i['model'].toJSON())\n",
    "        i['model'] = None\n",
    "    json.dump(obj, path, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_val(x):\n",
    "    a =['learning_rate', 'lr_decay_tau', 'alpha_momentum']\n",
    "    b =['adamax_learning_rate', 'exp_decay_rate_1', 'exp_decay_rate_2']\n",
    "    if x['adamax']:\n",
    "        for i in a:\n",
    "            if i in x.keys():\n",
    "                x[i] = None\n",
    "    else:\n",
    "        for i in b:\n",
    "            if i in x.keys():\n",
    "                x[i] = None   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = ['topology', 'stats',\n",
    " 'batch_size',\n",
    " 'min_epochs',\n",
    " 'max_epochs',\n",
    " 'patience',\n",
    " 'error_increase_tolerance',\n",
    " 'lambda_tikhonov',\n",
    " \n",
    " 'learning_rate',\n",
    " 'alpha_momentum',\n",
    " 'lr_decay_tau',\n",
    " \n",
    " 'adamax',\n",
    " 'adamax_learning_rate',\n",
    " 'exp_decay_rate_1',\n",
    " 'exp_decay_rate_2',\n",
    " \n",
    " 'mean_mean_euclidean_error',\n",
    " 'mean_mean_squared_error',\n",
    " 'var_mean_euclidean_error',\n",
    " 'var_mean_squared_error',\n",
    " 'mean_best_validation_training_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_files = ['bagging_model.csv']\n",
    "topologies = ['32_sigmoid']\n",
    "folder = '../data/gs_data/'\n",
    "topologies_dict = {}\n",
    "gs_results = []\n",
    "for i, f in enumerate(results_files):\n",
    "    if os.path.isfile(folder+ f):\n",
    "        dummy = pd.read_csv(folder + f)\n",
    "        topologies_dict[topologies[i]] = ast.literal_eval(dummy['topology'][0])\n",
    "        dummy['topology'] = topologies[i]\n",
    "        \n",
    "        gs_results.append(dummy)\n",
    "    \n",
    "\n",
    "\n",
    "orig_df = pd.concat(gs_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = [x for x in columns_order if x in orig_df.columns]\n",
    "orig_df = orig_df[columns_order]\n",
    "\n",
    "order_by = 'mean_mean_euclidean_error'\n",
    "orig_df.sort_values(by=[order_by], inplace=True)\n",
    "orig_df = orig_df.reset_index(drop=True)\n",
    "gs_results = orig_df.drop(['stats'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results = gs_results.apply(remove_useless_val, axis=1)\n",
    "for i in gs_results.columns[1:]:\n",
    "    gs_results[i] = gs_results[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = list(gs_results.columns)\n",
    "numerical_col.remove('topology')\n",
    "st_opt_col = ['learning_rate','lr_decay_tau','alpha_momentum']\n",
    "adamax_opt_col = ['adamax','adamax_learning_rate','exp_decay_rate_1','exp_decay_rate_2']\n",
    "metrics_col = [x for x in gs_results.columns if x.startswith(('var', 'mean'))]\n",
    "general_col = [item for item in list(gs_results.columns) if item not in st_opt_col and item not in metrics_col and item not in adamax_opt_col]\n",
    "tr_input_col = [item for item in list(gs_results.columns) if item not in metrics_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topology</th>\n",
       "      <th>stats</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>min_epochs</th>\n",
       "      <th>max_epochs</th>\n",
       "      <th>patience</th>\n",
       "      <th>error_increase_tolerance</th>\n",
       "      <th>lambda_tikhonov</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>alpha_momentum</th>\n",
       "      <th>lr_decay_tau</th>\n",
       "      <th>adamax</th>\n",
       "      <th>mean_mean_euclidean_error</th>\n",
       "      <th>mean_mean_squared_error</th>\n",
       "      <th>var_mean_euclidean_error</th>\n",
       "      <th>var_mean_squared_error</th>\n",
       "      <th>mean_best_validation_training_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32_sigmoid</td>\n",
       "      <td>{'adamax': False, 'exp_decay_rate_2': 0.999, '...</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.85</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "      <td>0.098424</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>5.119440e-07</td>\n",
       "      <td>0.009886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topology                                              stats  batch_size  \\\n",
       "0  32_sigmoid  {'adamax': False, 'exp_decay_rate_2': 0.999, '...           8   \n",
       "\n",
       "   min_epochs  max_epochs  patience  error_increase_tolerance  \\\n",
       "0         150         500         5                  0.000001   \n",
       "\n",
       "   lambda_tikhonov  learning_rate  alpha_momentum  lr_decay_tau  adamax  \\\n",
       "0     1.000000e-09           0.11            0.85           200   False   \n",
       "\n",
       "   mean_mean_euclidean_error  mean_mean_squared_error  \\\n",
       "0                   0.098424                 0.013096   \n",
       "\n",
       "   var_mean_euclidean_error  var_mean_squared_error  \\\n",
       "0                  0.000005            5.119440e-07   \n",
       "\n",
       "   mean_best_validation_training_error  \n",
       "0                             0.009886  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = np.random.default_rng(seed=None)\n",
    "def get_new_tr_vl(pattern_set, len_ds, gen):\n",
    "    return gen.choice(pattern_set, len_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting values:  {'training_set_len': 1000, 'minibatch_size': 8, 'max_epochs': 2, 'retrainig_es_error': 0.01087478308764089, 'error_increase_tolerance': 1e-06, 'patience': 5, 'min_epochs': 150, 'learning_rate': 0.01375, 'lr_decay_tau': 200, 'eta_tau': 0.0001375, 'lambda_tikhonov': 1e-09, 'alpha_momentum': 0.85, 'nesterov': False, 'adamax': False, 'adamax_learning_rate': 0.01, 'exp_decay_rate_1': 0.9, 'exp_decay_rate_2': 0.999, 'best_validation_training_error': inf, 'epochs': 0, 'total_train_time': datetime.timedelta(days=-1, seconds=86399, microseconds=999993), 'mean_epoch_train_time': 0, 'units_weights': {41: [], 40: [], 39: [], 38: [], 37: [], 36: [], 35: [], 34: [], 33: [], 32: [], 31: [], 30: [], 29: [], 28: [], 27: [], 26: [], 25: [], 24: [], 23: [], 22: [], 21: [], 20: [], 19: [], 18: [], 17: [], 16: [], 15: [], 14: [], 13: [], 12: [], 11: [], 10: [], 44: [], 43: [], 42: []}, 'units_weights_batch': {}, 'training_mean_squared_error': [], 'validation_mean_squared_error': [], 'training_pred_mean_squared_error': [], 'validation_pred_mean_squared_error': [], 'training_mean_euclidean_error': [], 'validation_mean_euclidean_error': [], 'training_pred_mean_euclidean_error': [], 'validation_pred_mean_euclidean_error': []}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] tr time: 0:00:02.692296 | mean_squared_error: tr=0.15416891298819488 val=-1 | | mean_euclidean_error: tr=0.34973740185633845 val=-1 | \n",
      "[2/2] tr time: 0:00:02.140128 | mean_squared_error: tr=0.11659703644101484 val=-1 | | mean_euclidean_error: tr=0.3048681437313312 val=-1 | \n",
      "starting values:  {'training_set_len': 1000, 'minibatch_size': 8, 'max_epochs': 2, 'retrainig_es_error': 0.01087478308764089, 'error_increase_tolerance': 1e-06, 'patience': 5, 'min_epochs': 150, 'learning_rate': 0.01375, 'lr_decay_tau': 200, 'eta_tau': 0.0001375, 'lambda_tikhonov': 1e-09, 'alpha_momentum': 0.85, 'nesterov': False, 'adamax': False, 'adamax_learning_rate': 0.01, 'exp_decay_rate_1': 0.9, 'exp_decay_rate_2': 0.999, 'best_validation_training_error': inf, 'epochs': 0, 'total_train_time': datetime.timedelta(days=-1, seconds=86399, microseconds=999996), 'mean_epoch_train_time': 0, 'units_weights': {41: [], 40: [], 39: [], 38: [], 37: [], 36: [], 35: [], 34: [], 33: [], 32: [], 31: [], 30: [], 29: [], 28: [], 27: [], 26: [], 25: [], 24: [], 23: [], 22: [], 21: [], 20: [], 19: [], 18: [], 17: [], 16: [], 15: [], 14: [], 13: [], 12: [], 11: [], 10: [], 44: [], 43: [], 42: []}, 'units_weights_batch': {}, 'training_mean_squared_error': [], 'validation_mean_squared_error': [], 'training_pred_mean_squared_error': [], 'validation_pred_mean_squared_error': [], 'training_mean_euclidean_error': [], 'validation_mean_euclidean_error': [], 'training_pred_mean_euclidean_error': [], 'validation_pred_mean_euclidean_error': []}\n",
      "[1/2] tr time: 0:00:02.260661 | mean_squared_error: tr=0.3121467549169524 val=-1 | | mean_euclidean_error: tr=0.48045105916217123 val=-1 | \n",
      "[2/2] tr time: 0:00:02.478480 | mean_squared_error: tr=0.09206514855545504 val=-1 | | mean_euclidean_error: tr=0.2660407410863024 val=-1 | \n",
      "starting values:  {'training_set_len': 1000, 'minibatch_size': 8, 'max_epochs': 2, 'retrainig_es_error': 0.01087478308764089, 'error_increase_tolerance': 1e-06, 'patience': 5, 'min_epochs': 150, 'learning_rate': 0.01375, 'lr_decay_tau': 200, 'eta_tau': 0.0001375, 'lambda_tikhonov': 1e-09, 'alpha_momentum': 0.85, 'nesterov': False, 'adamax': False, 'adamax_learning_rate': 0.01, 'exp_decay_rate_1': 0.9, 'exp_decay_rate_2': 0.999, 'best_validation_training_error': inf, 'epochs': 0, 'total_train_time': datetime.timedelta(days=-1, seconds=86399, microseconds=999995), 'mean_epoch_train_time': 0, 'units_weights': {41: [], 40: [], 39: [], 38: [], 37: [], 36: [], 35: [], 34: [], 33: [], 32: [], 31: [], 30: [], 29: [], 28: [], 27: [], 26: [], 25: [], 24: [], 23: [], 22: [], 21: [], 20: [], 19: [], 18: [], 17: [], 16: [], 15: [], 14: [], 13: [], 12: [], 11: [], 10: [], 44: [], 43: [], 42: []}, 'units_weights_batch': {}, 'training_mean_squared_error': [], 'validation_mean_squared_error': [], 'training_pred_mean_squared_error': [], 'validation_pred_mean_squared_error': [], 'training_mean_euclidean_error': [], 'validation_mean_euclidean_error': [], 'training_pred_mean_euclidean_error': [], 'validation_pred_mean_euclidean_error': []}\n",
      "[1/2] tr time: 0:00:02.241687 | mean_squared_error: tr=0.17891566949652662 val=-1 | | mean_euclidean_error: tr=0.36302094035681787 val=-1 | \n",
      "[2/2] tr time: 0:00:02.248598 | mean_squared_error: tr=0.08476344363044482 val=-1 | | mean_euclidean_error: tr=0.26192968723088667 val=-1 | \n"
     ]
    }
   ],
   "source": [
    "n_models = 3\n",
    "tr_len = 533\n",
    "max_epochs = 2\n",
    "mod = []\n",
    "\n",
    "met = [ErrorFunctions.mean_squared_error, ErrorFunctions.mean_euclidean_error, ]\n",
    "predictions_accumul_tr = []\n",
    "predictions_accumul_val = []\n",
    "predictions_accumul_tr = [np.zeros((max_epochs, training_len, TR_OUTPUT))]*2\n",
    "predictions_accumul_val = [np.zeros((max_epochs, test_len, TR_OUTPUT))]*2\n",
    "\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    tr = get_new_tr_vl(data_set_normalized, len(data_set_normalized), gen)\n",
    "    NN = NeuralNetwork(sigmoid_l1, -0.75, 0.75, True, (i + 54789))\n",
    "    stats = NN.train(training_set = data_set_normalized, \n",
    "                    validation_set = None, \n",
    "                    \n",
    "                    batch_size= 8, \n",
    "                    max_epochs= max_epochs, \n",
    "                    min_epochs= 150,\n",
    "                    retrainig_es_error = orig_df.iloc[0]['mean_best_validation_training_error'],\n",
    "                    patience = 5, \n",
    "                    error_increase_tolerance = 0.000001, \n",
    "                    \n",
    "                    lambda_tikhonov = 1.000000e-09, # off\n",
    "                    \n",
    "                    adamax = False,\n",
    "                    \n",
    "                    learning_rate = 0.11/8,\n",
    "                    lr_decay_tau = 200, # off\n",
    "                    eta_tau= (0.11/8)*0.01,\n",
    "                    alpha_momentum = 0.85, # off\n",
    "                    nesterov = False,\n",
    "                    \n",
    "                    metrics = [ErrorFunctions.mean_squared_error, ErrorFunctions.mean_euclidean_error, ], \n",
    "                    collect_data=True, \n",
    "                    collect_data_batch=False, \n",
    "                    verbose=True,\n",
    "                    \n",
    "                    dataset_agg = tr_normalized)\n",
    "    \n",
    "    for j, mes in enumerate(met):\n",
    "        \n",
    "        predictions_accumul_tr[j] += np.array(stats['training_pred_' + mes.__name__] + \n",
    "                                              [stats['training_pred_' + mes.__name__][-1]]*(max_epochs - stats['epochs']))\n",
    "        del stats['training_pred_' + mes.__name__]\n",
    "        \n",
    "        '''\n",
    "        predictions_accumul_val[j] += np.array(stats['validation_pred_' + mes.__name__] + \n",
    "                                              [stats['validation_pred_' + mes.__name__][-1]]*(max_epochs - stats['epochs']))\n",
    "        del stats['validation_pred_' + mes.__name__]'''\n",
    "    \n",
    "    \n",
    "    mod.append([NN, stats, tr])\n",
    "    with open('../data/net/models_6_ens_retr_jacopo/model_' + str(i) + '.json', 'w+') as file:\n",
    "            file.write(NN.toJSON())\n",
    "\n",
    "for i in range(len(met)):\n",
    "    predictions_accumul_tr[i] /= n_models\n",
    "    predictions_accumul_val[i] /= n_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_obj(list_mod):\n",
    "    interesting_model = []\n",
    "    for i in range(len(list_mod)):\n",
    "        \n",
    "        dummy = {}\n",
    "        dummy['nn_file_name'] = '../data/net/models_6_ens_retr_jacopo/model_' + str(i) + '.json'\n",
    "        dummy['index'] = i\n",
    "        dummy['model'] = list_mod[i][0]\n",
    "        dummy['stats'] = list_mod[i][1]\n",
    "        dummy['top_name'] = '32_sigmoid'\n",
    "        \n",
    "        dummy['std_prediction_tr'] = dummy['model'].predict_array(list_mod[i][2][:,:TR_INPUT])\n",
    "        dummy['std_prediction_test'] = dummy['model'].predict_array(test_normalized[:,:TR_INPUT])\n",
    "        \n",
    "        dummy['prediction_tr'] = scaler_out.inverse_transform(dummy['std_prediction_tr'])\n",
    "        dummy['prediction_test'] = scaler_out.inverse_transform(dummy['std_prediction_test'])\n",
    "\n",
    "        dummy['std_tr_error'] = ErrorFunctions.mean_euclidean_error(dummy['std_prediction_tr'], list_mod[i][2][:,TR_INPUT:])\n",
    "        dummy['std_test_error'] = ErrorFunctions.mean_euclidean_error(dummy['std_prediction_test'], test_normalized[:,TR_INPUT:])\n",
    "             \n",
    "        dummy['tr_error'] = ErrorFunctions.mean_euclidean_error(dummy['prediction_tr'], scaler_out.inverse_transform(list_mod[i][2][:,TR_INPUT:]))\n",
    "        dummy['test_error'] = ErrorFunctions.mean_euclidean_error(dummy['prediction_test'], test_set[:,TR_INPUT:])\n",
    "        interesting_model.append(dummy)\n",
    "        \n",
    "    return interesting_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = construct_obj(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/net/models_6_ens_retr_jacopo/models.json', 'w+') as file:\n",
    "    save_obj(models_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/net/models_6_ens_retr_jacopo/ens_curves_tr.json', 'w+') as file:\n",
    "    file.write(json.dumps(np.array(predictions_accumul_tr).tolist()))\n",
    "with open('../data/net/models_6_ens_retr_jacopo/ens_curves_test.json', 'w+') as file:\n",
    "    file.write(json.dumps(np.array(predictions_accumul_val).tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
